---
title: "Biostat 203B Homework 4"
subtitle: Due Mar 19 @ 11:59PM
author: "Olivia Golston"
output: 
  html_document:
    toc: true
    toc_depth: 4
---

```{r, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```
                      
Display machine information:
```{r}
sessionInfo()
```
Load database libraries and the tidyverse frontend:
```{r}
library(tidyverse)
library(lubridate)
library(miceRanger)
```

## Q1. Missing data

Through the Shiny app developed in HW3, we observe abundant missing values in the MIMIC-IV ICU cohort we created. In this question, we use multiple imputation to obtain a data set without missing values.

0. Read following tutorials on the R package miceRanger for imputation: <https://github.com/farrellday/miceRanger>, <https://cran.r-project.org/web/packages/miceRanger/vignettes/miceAlgorithm.html>.

    A more thorough book treatment of the practical imputation strategies is the book [*_Flexible Imputation of Missing Data_*](https://stefvanbuuren.name/fimd/) by Stef van Buuren. 

### Question 1.1
Explain the jargon MCAR, MAR, and MNAR.

**Solution**

MCAR, MAR, and MNAR are different classifications of missing data.

MCAR stands for Missing Completely at Random. This occurs when there is no systematic pattern in what data is present or missing. In the MIMIC-IV data, an example would be if a lab report got lost by a fluke, so a patient's measurements were missing. It is equally likely that this could happen to any patient, so the data is missing completely at random.

MAR stands for Missing at Random. This is when the missing data is more likely for certain subgroups of the study population, but is random within those groups. For example, it is possible that certain hospital departments are less meticulous about records keeping, so missing values - though completely random within the department - will be more common for patients in that particular department.

MNAR stands for Not Missing at Random. This occurs when the missing values are present due to a non-random mechanism. There are multiple ways this could happen in the MIMIC-IV data:
* Certain labs are only ordered for patients with particular conditions, so healthier patients may tend to have more missing values.
* Unconscious or otherwise incapacitated patients may not be able to provide answers to demographic questions, such as marital status.
* Certain departments may not collect certain pieces of data. 

Understanding which of these 3 classifications applies is necessary when determining how to address the missing data. 

Source: [Stef van Buuren's book](https://stefvanbuuren.name/fimd/sec-MCAR.html)


### Question 1.2
Explain in a couple of sentences how the Multiple Imputation by Chained Equations (MICE) work.

**Solution**

MICE uses available values in a dataset to make the best possible guess about what the missing values would have been.

For the MIMIC-IV data, it will work as follows:

1. Missing values are given a random value - these values will be replaced during the imputation process.
2. For a column of interest (say, `Heart Rate`), the missing values are imputed using the joint correlation between the other variables and `Heart Rate`, using a Random Forest. A person with missing `Heart Rate`, but other lab/chart measurements and demographic characteristics associated with an elevated heart rate will be given a high value.
3. This is repeated for other columns with missing data. 
4. This process (steps 2-3) may be repeated a few times to reach convergence. 


### Question 1.3 
Perform a data quality check of the ICU stays data. Discard variables with substantial missingness, say >5000 `NA`s. Replace apparent data entry errors by `NA`s.

**Solution**

First, read in data to `icu_cohort`:
```{r}
icu_cohort <- readRDS("/home/OLIVIAGOLSTON/biostat-203b-2021-winter/hw3/mimiciv_shiny/icu_cohort.rds")
```


```{r}
missing <- as_tibble(colSums(is.na(icu_cohort))) %>%
  rename(number_missing = value) %>%
  mutate(measure = colnames(icu_cohort)) %>%
  arrange(desc(number_missing)) %>%
  print()
```

There are a lot of missing values. No patients have `dod` measurements, and most (fortunately!) do not have `deathtime`. These variables are related to our outcome of interest in Question 2, but the indicator variable `death_in_month` will be used, so `dod` and `deathtime` can be dropped.

Among the chart and lab measurements, `arterial_blood_pressure_systolic`, `arterial_blood_pressure_mean`, and `lactate` all have a lot of missing values. These must not be measurements that are taken for every ICU patient, so this is a likely a "Missing Not at Random" scenario, and imputation would not be appropriate. 

Finally, `edregtime` and `edouttime` are missing for a lot of people. These aren't of much interest, anyway, and can be dropped. I'm also going to drop any variables that won't be used for the final analysis, so anything other than lab measurements, chart measurements, gender, marital status, age, and ethnicity. 


The remaining variables are relevant for the final analysis and have <5000 missing values, so will be kept, and the missing values will be imputed. It's not known why these values of missing, so it may not truly be random. However, for the purposes of this project, the missingness will be viewed as random. 

Drop the variables: 
```{r}
drop_vars <- c("dod", "deathtime", "arterial_blood_pressure_systolic",
               "arterial_blood_pressure_mean", "lactate", "edregtime",
               "edouttime", "subject_id", "stay_id", "hadm_id", "intime",
               "outtime", "last_careunit", "first_careunit", "los",
               "dischtime", "admittime", "language", "anchor_year_group",
               "anchor_year", "anchor_age", "hospital_expire_flag", "insurance",
               "discharge_location", "admission_location", "admission_type")

icu_cohort <- icu_cohort %>%
  select(-all_of(drop_vars)) %>% 
  print()


colnames(icu_cohort)
```

Now there are only the demographic characteristics of interest, lab measurements, chart measurements, and the outcome variable in the data. 


There also appear to be a good deal of data entry errors. Using my Shiny app, I looked for any extreme outliers or otherwise unusual values. 

One challenge in this process is that I don't have the medical knowledge to judge whether the extreme values are legitimate readings or not. If the extreme outliers are accurate, it's probably better to keep them, since they could be a sign of very poor health and might help predict death. On the other hand, if they are illegitimate, they could skew the results. 

I did my best to research what physically possible values are and only remove ones that I believe are truly unrealistic and the result of data entry errors. However, it's possible I was too conservative with my selections. However, I wanted to ensure that valid information was not thrown out. In a real study, I would consult with an expert before deciding which values to discard.

In my Shiny app, I found a few variables with especially odd values, outlined below.

**Heart Rate**
```{r}
icu_cohort %>%
  filter(heart_rate > 200 | heart_rate == 0) %>% 
  print()
```
I'm skeptical about the numbers in the 200's, but I think 941 is the only unambiguous data entry error. Heart rates of 0 are also suspicious, but these may be cases where someone flatlined but then was (potentially) resuscitated, so I won't discard those values. 


**Mean Blood Pressure**
```{r}
icu_cohort %>%
  filter(non_invasive_blood_pressure_mean > 300 | non_invasive_blood_pressure_mean < 25) %>%
  select(non_invasive_blood_pressure_systolic, 
         non_invasive_blood_pressure_mean, heart_rate) %>%
  print()

```
All of these values are unrealistically high, so should be discarded. There are also suspicious values below 30, but I'm not sure if those are truly data entry errors, or if they indicate a significant medical event such as severe blood loss or a heart event, so I don't discard those.

**Systolic Blood Pressure**
```{r}
icu_cohort %>%
  filter(non_invasive_blood_pressure_systolic > 300) %>%
  select(non_invasive_blood_pressure_systolic, 
         non_invasive_blood_pressure_mean) %>%
  print()

```
These values are unrealistically high. For reasons listed above, aberrantly low values were not discarded.

**Temperature**
```{r}
icu_cohort %>%
  filter(temperature_fahrenheit < 90) %>%
  select(heart_rate, temperature_fahrenheit, respiratory_rate) %>%
  print()
```
There is a small cluster of temperatures around 37, which likely corresponds to Celcius values rather than Fahrenheit. Though I could probably deduce what the true measurement was, I set all of these to `NA`, along with any other illogically low values (<80). Values in the 80s might also be mistakes, but it's hard to be sure, since they could be cases of hypothermia. 


**Calcium**
```{r}
icu_cohort %>%
  filter(calcium > 18 | calcium < 1) %>%
  print()
```
The value of 43 seems unrealistic, and I'm also skeptical about the 27.5. These are so much larger than the other values, that it seems unlikely they are legitimate. Even if they are real, they are so rare that it's not really applicable to the wider population. Values below 5 are rare, but I suspect values of 0 may be data entry errors.

**Creatinine**
```{r}
icu_cohort %>%
  filter(creatinine > 20) %>%
  print()
```
Based on the Shiny app, values above 10 are unusual, but there is a marked dropoff at 20-25. 

**Magnesium**
```{r}
icu_cohort %>%
  filter(magnesium > 10) %>%
  select(magnesium) %>%
  print()
```
These values are much larger than all other values for magnesium.  Again, even if they are real, any trends likely wouldn't generalize to the larger population. 

**Glucose**
```{r}
summary(icu_cohort$glucose)

icu_cohort %>%
  filter(glucose > 1000) %>%
  arrange(desc(glucose)) %>%
  select(glucose) %>%
  print()
```
Glucose has a very large range, and there are extreme high and low values. I suspect this is related to diabetic crises and may represent real values. However, even among the extreme high values, 2440 seems unusual, so I decided to discard it. 


**Respiratory Rate**
```{r}
icu_cohort %>%
  filter(respiratory_rate > 50) %>%
  arrange(desc(respiratory_rate)) %>%
  select(respiratory_rate) %>%
  print()
```
Respiratoy rate measurements above 20 are considered elevated. Values above 50 seem questionable, but I think 80 is a safe cutoff for assuming errors, since there is a "cluster" of values in the 67-73 range. I decided not to discard low values, since there are reasons why patients may have a low respiratory rate (choking, dead, requiring mechanical ventilation, etc).

**Potassium**
```{r}
icu_cohort %>%
  filter(potassium > 8) %>%
  arrange(desc(potassium)) %>%
  select(potassium) %>%
  print()
```
13.0 is extreme, even among the extreme values.

**Sodium**
```{r}
icu_cohort %>%
  filter(sodium < 100) %>%
  select(sodium) %>%
  print()
```
Though extreme, there is a cluster of values in the 90's, so these may be legitimate. I decided not to discard. 

**WBC**
Has some extreme high/low values, but they might be feasible in cases of cancer/severe infections, so I decided not to discard anything.

**Ethnicity**
```{r}
icu_cohort %>%
  group_by(ethnicity) %>%
  count() %>%
  print()
```
For ethnicity, there are "NA" values in hiding - "Unknown" and "Unable to obtain." These values shouldn't be predictivive, but multiple imputation also seems a bit questionable. I decided to leave these values as is. 


Replace the identified aberrant values with `NA`. I also renamed a few variables, for convenience:
```{r}
cleaned_cohort <- icu_cohort %>%
  rename(systolic_bp = non_invasive_blood_pressure_systolic,
         mean_bp = non_invasive_blood_pressure_mean,
         temp = temperature_fahrenheit,
         resp_rate = respiratory_rate) %>%
  mutate(heart_rate = ifelse(heart_rate == 941, NA, heart_rate),
         systolic_bp = ifelse(systolic_bp > 300, NA, systolic_bp),
         mean_bp = ifelse(mean_bp > 300, NA, mean_bp),
         temp = ifelse(temp < 80, NA, temp),
         calcium = ifelse(calcium > 25 | calcium < 1, NA, calcium),
         creatinine = ifelse(creatinine > 25, NA, creatinine),
         magnesium = ifelse(magnesium > 10, NA, magnesium),
         glucose = ifelse(glucose > 2000, NA, glucose),
         potassium = ifelse(potassium == 13, NA, potassium),
         resp_rate = ifelse(resp_rate > 80, NA, resp_rate))
```



Double check some of the changes: 
```{r}
cleaned_cohort %>%
  filter(systolic_bp > 300 | temp < 80 | heart_rate > 800 | mean_bp > 300) %>%
  print()

```

### Question 1.4
Impute missing values by `miceRanger` (request $m=3$ datasets). This step is very computational intensive. Make sure to save the imputation results as a file.

**Solution**
```{r eval = F}
miceObj <- miceRanger(
    cleaned_cohort, 
    m = 3,
    max.depth = 10,
    returnModels = TRUE,
    verbose=FALSE
)

saveRDS(miceObj, "hw4_results.rds")
```


### Question 1.5
Make imputation diagnostic plots and explain what they mean.

**Solution**
```{r}
miceObj = readRDS("hw4_results.rds")
```


```{r}
plotDistributions(miceObj, vars='allNumeric')
```


```{r}
plotCorrelations(miceObj,vars='allNumeric')
```



```{r}
plotVarConvergence(miceObj,vars='allNumeric')
```




```{r}
plotModelError(miceObj,vars='allNumeric')
```

### Question 1.6
Obtain a complete data set by averaging the 3 imputed data sets.

**Solution**
```{r}
dataList <- completeData(miceObj)
dataList

```




Average the 3 imputed datasets: 
```{r}
data1 <- model.matrix(~., dataList[[1]])
data2 <- model.matrix(~., dataList[[2]])
data3 <- model.matrix(~., dataList[[3]])

final_data_matrix <- (data1 + data2 + data3)/3
```

## Q2. Predicting 30-day mortality

Develop at least two analytic approaches for predicting the 30-day mortality of patients admitted to ICU using demographic information (gender, age, marital status, ethnicity), first lab measurements during ICU stay, and first vital measurements during ICU stay. For example, you can use (1) logistic regression (`glm()` function), (2) logistic regression with lasso penalty (glmnet package), (3) random forest (randomForest package), or (4) neural network.

1. Partition data into 80% training set and 20% test set. Stratify partitioning according the 30-day mortality status.

2. Train the models using the training set.

3. Compare model prediction performance on the test set.


Convert back to a data frame and display possible features.
```{r}
final_data <- as.data.frame(final_data_matrix) %>%
  select(-c("(Intercept)"))

colnames(final_data)
```


### Solution: Logistic Regression

First, 80% of each outcome (death in 30 days or survived 30 days) into a training set, and the remaining 20% into the testing set. The partitioning was stratified based on outcome so that the probability of death in the training set would not randomly be too high or low. 
```{r}
dead <- final_data %>%
  filter(death_in_monthTRUE == 1)

survived <- final_data %>%
  filter(death_in_monthTRUE == 0)

set.seed(42)
dead_train <- dead %>%
  sample_frac(.8) 

dead_test <- dead %>%
  anti_join(dead_train)

survive_train <- survived %>%
  sample_frac(.8)

survive_test <- survived %>%
  anti_join(survive_train)


test <- rbind(survive_test, dead_test)
rows <- sample(nrow(test))
test <- test[rows, ]

train <- rbind(survive_train, dead_train)
rows <- sample(nrow(train))
train <- train[rows, ]

x_train <- train %>% 
  select(-death_in_monthTRUE) %>% 
  scale()

y_train <- train$death_in_monthTRUE

x_test <- test %>% 
  select(-death_in_monthTRUE) %>% 
  scale()

y_test <- test$death_in_monthTRUE

```


```{r}
mylogit <- glm(death_in_monthTRUE ~ ., data = train, family = "binomial")
mylogit
```

```{r}
library(glmnet)
fit = glmnet(x_train, y_train, family = "binomial")

```

```{r}
plot(fit)
```

```{r}
cvfit = cv.glmnet(x_train, y_train, family = "binomial")
plot(cvfit)
coef(cvfit, s = "lambda.1se")
```
```{r}
lassopredict <- predict(cvfit, newx = x_test, type = "class", s = "lambda.min")
```

```{r}
table(factor(lassopredict, 
      levels=min(test$death_in_monthTRUE):max(test$death_in_monthTRUE)),
      factor(test$death_in_monthTRUE, levels=min(test$death_in_monthTRUE):max(test$death_in_monthTRUE)))

```


### Solution: Neural Net
```{r}
detach("package:miceRanger", unload = TRUE)
library(keras)
```

```{r}
nn_survive_train <- survive_train 
 #%>%  sample_frac(.5)

nntrain <- rbind(nn_survive_train, dead_train)
rows <- sample(nrow(nntrain))
nntrain <- nntrain[rows, ]

x_train <- nntrain %>% 
  select(-death_in_monthTRUE) %>% 
  scale()

y_train <- nntrain$death_in_monthTRUE

dim(x_train)
```


```{r}
sum(y_train)/length(y_train)
sum(y_test)/length(y_test)
```

```{r}
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 256, activation = 'relu', input_shape = c(27)) %>% 
  layer_dropout(rate = 0.3) %>% 
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>% 
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>% 
  layer_dense(units = 1, activation = 'sigmoid')
summary(model)

```


```{r}
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)
```



```{r}
history <- model %>% fit(
  x_train, y_train, 
  epochs = 20, batch_size = 32, 
  validation_split = 0.3
)
```


```{r}
plot(history)
```


```{r}
model %>% 
  evaluate(x_test, y_test)
```

```{r}
predictions <- model %>% 
  predict(x_test) > 0.5 

predictions <- as.numeric(predictions)
```


```{r}
table(predictions, y_test)
```


